{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 11298\n",
      "../input/crack-seg/crack_segmentation_dataset/images/CFD_001.jpg | ../input/crack-seg/crack_segmentation_dataset/masks/CFD_001.jpg\n",
      "oi\n",
      "../input/crack-seg/crack_segmentation_dataset/images/CFD_002.jpg | ../input/crack-seg/crack_segmentation_dataset/masks/CFD_002.jpg\n",
      "oi\n",
      "../input/crack-seg/crack_segmentation_dataset/images/CFD_003.jpg | ../input/crack-seg/crack_segmentation_dataset/masks/CFD_003.jpg\n",
      "oi\n",
      "../input/crack-seg/crack_segmentation_dataset/images/CFD_004.jpg | ../input/crack-seg/crack_segmentation_dataset/masks/CFD_004.jpg\n",
      "oi\n",
      "../input/crack-seg/crack_segmentation_dataset/images/CFD_005.jpg | ../input/crack-seg/crack_segmentation_dataset/masks/CFD_005.jpg\n",
      "oi\n",
      "../input/crack-seg/crack_segmentation_dataset/images/CFD_006.jpg | ../input/crack-seg/crack_segmentation_dataset/masks/CFD_006.jpg\n",
      "oi\n",
      "../input/crack-seg/crack_segmentation_dataset/images/CFD_007.jpg | ../input/crack-seg/crack_segmentation_dataset/masks/CFD_007.jpg\n",
      "oi\n",
      "../input/crack-seg/crack_segmentation_dataset/images/CFD_008.jpg | ../input/crack-seg/crack_segmentation_dataset/masks/CFD_008.jpg\n",
      "oi\n",
      "../input/crack-seg/crack_segmentation_dataset/images/CFD_009.jpg | ../input/crack-seg/crack_segmentation_dataset/masks/CFD_009.jpg\n",
      "oi\n",
      "../input/crack-seg/crack_segmentation_dataset/images/CFD_010.jpg | ../input/crack-seg/crack_segmentation_dataset/masks/CFD_010.jpg\n",
      "oi\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "input_dir = \"../input/crack-seg/crack_segmentation_dataset/images\"\n",
    "target_dir = \"../input/crack-seg/crack_segmentation_dataset/masks\"\n",
    "img_size = (448, 448)\n",
    "num_classes = 1\n",
    "batch_size = 8\n",
    "\n",
    "input_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(input_dir, fname)\n",
    "        for fname in os.listdir(input_dir)\n",
    "        if fname.endswith(\".jpg\")\n",
    "    ]\n",
    ")\n",
    "target_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(target_dir, fname)\n",
    "        for fname in os.listdir(target_dir)\n",
    "        if fname.endswith(\".jpg\") and not fname.startswith(\".\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Number of samples:\", len(input_img_paths))\n",
    "\n",
    "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
    "    print(input_path, \"|\", target_path)\n",
    "    print('oi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from PIL import Image\n",
    "\n",
    "class Cracks(keras.utils.Sequence):\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = target_img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
    "        x = np.zeros((batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        for j, path in enumerate(batch_input_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size)\n",
    "            img_arr = np.asarray(img)\n",
    "            img_arr = img_arr.astype(np.float32)\n",
    "            img_arr = img_arr/255.0\n",
    "            x[j] = img_arr\n",
    "        y = np.zeros((batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
    "            y[j] = np.expand_dims(img, 2)/255\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Split our img paths into a training and a validation set\n",
    "val_samples = 1000\n",
    "random.Random(42).shuffle(input_img_paths)\n",
    "random.Random(42).shuffle(target_img_paths)\n",
    "train_input_img_paths = input_img_paths[:-val_samples]\n",
    "train_target_img_paths = target_img_paths[:-val_samples]\n",
    "val_input_img_paths = input_img_paths[-val_samples:]\n",
    "val_target_img_paths = target_img_paths[-val_samples:]\n",
    "\n",
    "# Instantiate data Sequences for each split\n",
    "train_gen = Cracks(\n",
    "    batch_size, img_size, train_input_img_paths, train_target_img_paths\n",
    ")\n",
    "val_gen = Cracks(batch_size, img_size, val_input_img_paths, val_target_img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model('../input/model-2/cracks_segmentation_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "val_gen = Cracks(batch_size, img_size, val_input_img_paths, val_target_img_paths)\n",
    "\n",
    "val_preds = model.predict(val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds_norm = preds*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds_norm_thres = (val_preds_norm > 51).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(y_true, y_pred):\n",
    "    return (2 * (y_true * y_pred).sum() + 1e-15) / (y_true.sum() + y_pred.sum() + 1e-15)\n",
    "\n",
    "ix = 0\n",
    "dice_avg = 0\n",
    "for batch in val_gen:\n",
    "    for image in batch[1]:\n",
    "        y_true = (image > 0).astype(np.uint8)\n",
    "        y_pred = val_preds_norm_thres[ix]\n",
    "        dice_avg += dice(y_true, y_pred)\n",
    "        ix += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Dice: 0.5765572559091258\n"
     ]
    }
   ],
   "source": [
    "print(\"Avg. Dice:\", dice_avg/1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
