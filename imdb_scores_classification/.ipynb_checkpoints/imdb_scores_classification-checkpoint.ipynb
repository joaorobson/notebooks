{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bodtn5BX6a2x"
   },
   "source": [
    "# Classificação das notas de filme - IMDb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqMU5ZR66a20"
   },
   "source": [
    "IMDb é uma base de dados online com informações relativas a filmes e programas de TV. \n",
    "No site, podem ser encontradas avaliações de filmes e as suas respectivas notas.\n",
    "\n",
    "Com base nesse dado, é possível criar um modelo capaz de, dado uma crítica/avalição do filme, sabermos sua avaliação foi positiva ou negativa.\n",
    "\n",
    "O dataset e informações adicionais podem ser encontrados nesse link também:\n",
    "https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgNRKp5pj_fj"
   },
   "source": [
    "### Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "uDnUy43QsbY6",
    "outputId": "0c4a86c0-0d09-4aad-ac61-8dce91a31ffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
      "\r",
      "\u001b[K     |█▍                              | 10kB 18.4MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 20kB 1.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 30kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 40kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 51kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 61kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 71kB 3.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 81kB 3.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 92kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 102kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 112kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 122kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 133kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 143kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 153kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 163kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 174kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 184kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 194kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 204kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 215kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 225kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 235kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 245kB 2.9MB/s \n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.1.1\n",
      "Collecting tqdm==4.33.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/56/60a5b1c2e634d8e4ff89c7bab47645604e19658f448050a21facffd43796/tqdm-4.33.0-py2.py3-none-any.whl (50kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 1.7MB/s \n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "  Found existing installation: tqdm 4.28.1\n",
      "    Uninstalling tqdm-4.28.1:\n",
      "      Successfully uninstalled tqdm-4.28.1\n",
      "Successfully installed tqdm-4.33.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "tqdm"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "!pip install tqdm==4.33.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "pWKCnicJkBC2",
    "outputId": "550d499e-1e8d-450c-fb0a-d6a1178cb62e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import unidecode\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "sns.set()\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztAFM6wukIEK"
   },
   "source": [
    "### Leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sqkdv3zkHr9"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "3lExgMy9kdz1",
    "outputId": "4210c4b6-78e0-40b4-dbff-ae30729a99d9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "FQtbVA1hlDpw",
    "outputId": "cf8d4b3b-8731-4d19-b43d-a92289ebefab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      "review       50000 non-null object\n",
      "sentiment    50000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "J9LlWPgTk9Tt",
    "outputId": "c9f05eb9-4ec7-4850-a3d3-7146e5ef1e9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFHZFhGBmsLs"
   },
   "source": [
    "O dado está dividido em exatamente 25000 samples negativos e 25000 positivos. Não há dados faltantes também. Então não há preocupação em balanceamento dos dados ou necessidade de under/oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ha2bXHc31jLI"
   },
   "outputs": [],
   "source": [
    "def get_vocab_size(corpus):\n",
    "    vocab = set()\n",
    "    for review in corpus:\n",
    "        for word in review.split():\n",
    "            vocab.add(word)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "GllrSId62D-c",
    "outputId": "8209d41e-d0b7-4661-b3b0-452aa86c7424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário antes do pré-processamento: 438729\n"
     ]
    }
   ],
   "source": [
    "print('Tamanho do vocabulário antes do pré-processamento:', len(get_vocab_size(df.review)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJX68S0am_ZN"
   },
   "source": [
    "### Pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hn_6TA0ToYeX"
   },
   "source": [
    "Como se pôde observar, há tags HTML inseridas no texto. Primeiramente, elas serão retiradas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfoOwVGdlFrC"
   },
   "outputs": [],
   "source": [
    "HTML_REGEX = re.compile(r'<.*?>')\n",
    "\n",
    "def remove_html_tags(text):    \n",
    "    text = re.sub(HTML_REGEX, '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "EAI6g4GoobCn",
    "outputId": "d97470b5-07f2-4577-a5db-96ebb8a84cf3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>A wonderful little production. The filming tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  ...                                     cleaned_review\n",
       "0      One of the other reviewers has mentioned that ...  ...  One of the other reviewers has mentioned that ...\n",
       "1      A wonderful little production. <br /><br />The...  ...  A wonderful little production. The filming tec...\n",
       "2      I thought this was a wonderful way to spend ti...  ...  I thought this was a wonderful way to spend ti...\n",
       "3      Basically there's a family where a little boy ...  ...  Basically there's a family where a little boy ...\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  ...  Petter Mattei's \"Love in the Time of Money\" is...\n",
       "...                                                  ...  ...                                                ...\n",
       "49995  I thought this movie did a down right good job...  ...  I thought this movie did a down right good job...\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  ...  Bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997  I am a Catholic taught in parochial elementary...  ...  I am a Catholic taught in parochial elementary...\n",
       "49998  I'm going to have to disagree with the previou...  ...  I'm going to have to disagree with the previou...\n",
       "49999  No one expects the Star Trek movies to be high...  ...  No one expects the Star Trek movies to be high...\n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_review'] = df.review.apply(remove_html_tags)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmC5wdMGpW7i"
   },
   "source": [
    "O próximo passo é realizar o restante do pré-processamento do texto, passando o texto para lowercase, removendo símbolos, pontuações e acentuação e stopwords. O objetivo dessa fase é remover ruído do corpus assim como palavras que possuem uma frequência alta na língua inglesa apesar de possuir pouca influência na semântica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pO1rhL_ppWxA"
   },
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('portuguese'))\n",
    "\n",
    "# regex para remove todos caracteres, exceto letras, números e espaços\n",
    "WORDS_AND_NUMBERS_REGEX = re.compile(r'[^a-zA-Z0-9 ]+')\n",
    "\n",
    "def pre_process(text):\n",
    "    # Texto para lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remover acentos\n",
    "    text = unidecode.unidecode(text)\n",
    "\n",
    "    # Remover símbolos e pontuação\n",
    "    text = re.sub(WORDS_AND_NUMBERS_REGEX, '', text)\n",
    "\n",
    "    # Remover stopwords\n",
    "    text = text.split()\n",
    "    text = [word for word in text if word not in STOPWORDS]\n",
    "\n",
    "    return ' '.join(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "9ndraviDUD2L",
    "outputId": "f46f01af-ef9a-4701-84b7-387e3fac8d61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ser ruim alguma coisa primeiro passo tornar bom alguma coisa'"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_process(\"Ser ruim em alguma coisa é o primeiro passo para se tornar bom em alguma coisa.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "NK16tVPNwJIy",
    "outputId": "f482acbf-0617-4aca-e795-5371579355b9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one reviewers mentioned watching 1 oz episode ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically theres family little boy jake thinks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought movie right good job wasnt creative or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>bad plot bad dialogue bad acting idiotic direc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "      <td>catholic taught parochial elementary schools n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "      <td>im going disagree previous comment side maltin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "      <td>one expects star trek movies high art fans exp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  ...                                     cleaned_review\n",
       "0      One of the other reviewers has mentioned that ...  ...  one reviewers mentioned watching 1 oz episode ...\n",
       "1      A wonderful little production. <br /><br />The...  ...  wonderful little production filming technique ...\n",
       "2      I thought this was a wonderful way to spend ti...  ...  thought wonderful way spend time hot summer we...\n",
       "3      Basically there's a family where a little boy ...  ...  basically theres family little boy jake thinks...\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  ...  petter matteis love time money visually stunni...\n",
       "...                                                  ...  ...                                                ...\n",
       "49995  I thought this movie did a down right good job...  ...  thought movie right good job wasnt creative or...\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  ...  bad plot bad dialogue bad acting idiotic direc...\n",
       "49997  I am a Catholic taught in parochial elementary...  ...  catholic taught parochial elementary schools n...\n",
       "49998  I'm going to have to disagree with the previou...  ...  im going disagree previous comment side maltin...\n",
       "49999  No one expects the Star Trek movies to be high...  ...  one expects star trek movies high art fans exp...\n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_review'] = df.cleaned_review.apply(pre_process)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "51FvZaPW1e_Y",
    "outputId": "04eebd4b-bddb-4f8d-ee19-4a4dd462e912"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário após pré-processamento inicial: 221408\n"
     ]
    }
   ],
   "source": [
    "print('Tamanho do vocabulário após pré-processamento inicial:', len(get_vocab_size(df.cleaned_review)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRxEqd7O1B-o"
   },
   "source": [
    "O próximo passo é realizar os processos de lemmatization e stemming. Eles também têm o objetivo de tirar ruído do texto, mas com a intenção de agrupar termos similares. O primeiro se refere a agrupar variações de uma mesma palavra ou sinônimos em um mesmo termo, como bons e boa em bom. O segundo se trata do mantimento do radical das palavras, como em corrida e corro, que virariam \"corr\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "wM0GRlrPryKe",
    "outputId": "417e2e9f-8305-44cb-dcb5-e1f9e219599c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt_core_news_sm==2.1.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.1.0/pt_core_news_sm-2.1.0.tar.gz (12.8MB)\n",
      "\u001b[K     |████████████████████████████████| 12.9MB 845kB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pt-core-news-sm\n",
      "  Building wheel for pt-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pt-core-news-sm: filename=pt_core_news_sm-2.1.0-cp36-none-any.whl size=12843677 sha256=bdbe04d39468c06487b4b1413494bcd4e65ab054f1fc22123f39e2c3d0d49e8d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fwrq4e89/wheels/a3/8f/c1/f036e3a7f1aa44fb06a534c6c4b1c2b773f101fdb1f163c08c\n",
      "Successfully built pt-core-news-sm\n",
      "Installing collected packages: pt-core-news-sm\n",
      "Successfully installed pt-core-news-sm-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download pt_core_news_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VYb4a--rhrc"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8__iUetrjYx"
   },
   "outputs": [],
   "source": [
    "a = nlp(\"tudo que é pequeno é só uma versão menor de algo grande\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S68WqrdBrjHS",
    "outputId": "1524c4ce-23b6-488d-c1b0-4d67cabee760"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as',\n",
       " 'até',\n",
       " 'com',\n",
       " 'como',\n",
       " 'da',\n",
       " 'das',\n",
       " 'de',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'dele',\n",
       " 'deles',\n",
       " 'depois',\n",
       " 'do',\n",
       " 'dos',\n",
       " 'e',\n",
       " 'ela',\n",
       " 'elas',\n",
       " 'ele',\n",
       " 'eles',\n",
       " 'em',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'eram',\n",
       " 'essa',\n",
       " 'essas',\n",
       " 'esse',\n",
       " 'esses',\n",
       " 'esta',\n",
       " 'estamos',\n",
       " 'estas',\n",
       " 'estava',\n",
       " 'estavam',\n",
       " 'este',\n",
       " 'esteja',\n",
       " 'estejam',\n",
       " 'estejamos',\n",
       " 'estes',\n",
       " 'esteve',\n",
       " 'estive',\n",
       " 'estivemos',\n",
       " 'estiver',\n",
       " 'estivera',\n",
       " 'estiveram',\n",
       " 'estiverem',\n",
       " 'estivermos',\n",
       " 'estivesse',\n",
       " 'estivessem',\n",
       " 'estivéramos',\n",
       " 'estivéssemos',\n",
       " 'estou',\n",
       " 'está',\n",
       " 'estávamos',\n",
       " 'estão',\n",
       " 'eu',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'for',\n",
       " 'fora',\n",
       " 'foram',\n",
       " 'forem',\n",
       " 'formos',\n",
       " 'fosse',\n",
       " 'fossem',\n",
       " 'fui',\n",
       " 'fôramos',\n",
       " 'fôssemos',\n",
       " 'haja',\n",
       " 'hajam',\n",
       " 'hajamos',\n",
       " 'havemos',\n",
       " 'hei',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houver',\n",
       " 'houvera',\n",
       " 'houveram',\n",
       " 'houverei',\n",
       " 'houverem',\n",
       " 'houveremos',\n",
       " 'houveria',\n",
       " 'houveriam',\n",
       " 'houvermos',\n",
       " 'houverá',\n",
       " 'houverão',\n",
       " 'houveríamos',\n",
       " 'houvesse',\n",
       " 'houvessem',\n",
       " 'houvéramos',\n",
       " 'houvéssemos',\n",
       " 'há',\n",
       " 'hão',\n",
       " 'isso',\n",
       " 'isto',\n",
       " 'já',\n",
       " 'lhe',\n",
       " 'lhes',\n",
       " 'mais',\n",
       " 'mas',\n",
       " 'me',\n",
       " 'mesmo',\n",
       " 'meu',\n",
       " 'meus',\n",
       " 'minha',\n",
       " 'minhas',\n",
       " 'muito',\n",
       " 'na',\n",
       " 'nas',\n",
       " 'nem',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nossa',\n",
       " 'nossas',\n",
       " 'nosso',\n",
       " 'nossos',\n",
       " 'num',\n",
       " 'numa',\n",
       " 'não',\n",
       " 'nós',\n",
       " 'o',\n",
       " 'os',\n",
       " 'ou',\n",
       " 'para',\n",
       " 'pela',\n",
       " 'pelas',\n",
       " 'pelo',\n",
       " 'pelos',\n",
       " 'por',\n",
       " 'qual',\n",
       " 'quando',\n",
       " 'que',\n",
       " 'quem',\n",
       " 'se',\n",
       " 'seja',\n",
       " 'sejam',\n",
       " 'sejamos',\n",
       " 'sem',\n",
       " 'serei',\n",
       " 'seremos',\n",
       " 'seria',\n",
       " 'seriam',\n",
       " 'será',\n",
       " 'serão',\n",
       " 'seríamos',\n",
       " 'seu',\n",
       " 'seus',\n",
       " 'somos',\n",
       " 'sou',\n",
       " 'sua',\n",
       " 'suas',\n",
       " 'são',\n",
       " 'só',\n",
       " 'também',\n",
       " 'te',\n",
       " 'tem',\n",
       " 'temos',\n",
       " 'tenha',\n",
       " 'tenham',\n",
       " 'tenhamos',\n",
       " 'tenho',\n",
       " 'terei',\n",
       " 'teremos',\n",
       " 'teria',\n",
       " 'teriam',\n",
       " 'terá',\n",
       " 'terão',\n",
       " 'teríamos',\n",
       " 'teu',\n",
       " 'teus',\n",
       " 'teve',\n",
       " 'tinha',\n",
       " 'tinham',\n",
       " 'tive',\n",
       " 'tivemos',\n",
       " 'tiver',\n",
       " 'tivera',\n",
       " 'tiveram',\n",
       " 'tiverem',\n",
       " 'tivermos',\n",
       " 'tivesse',\n",
       " 'tivessem',\n",
       " 'tivéramos',\n",
       " 'tivéssemos',\n",
       " 'tu',\n",
       " 'tua',\n",
       " 'tuas',\n",
       " 'tém',\n",
       " 'tínhamos',\n",
       " 'um',\n",
       " 'uma',\n",
       " 'você',\n",
       " 'vocês',\n",
       " 'vos',\n",
       " 'à',\n",
       " 'às',\n",
       " 'é',\n",
       " 'éramos'}"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1zsOKklJWMq"
   },
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3y3aENe0fRo"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def lemmatize(text):\n",
    "  text = text.split()\n",
    "  lemmatized_text = []\n",
    "  for word in text:\n",
    "      lemmatized_text.append(lemmatizer.lemmatize(word))\n",
    "  return ' '.join(lemmatized_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "XGL96s3eJxhJ",
    "outputId": "a3b12bef-3a35-4099-8bbe-f64bed7f1abb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:22<00:00, 2204.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one reviewer mentioned watching 1 oz episode y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically there family little boy jake think t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought movie right good job wasnt creative or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>bad plot bad dialogue bad acting idiotic direc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "      <td>catholic taught parochial elementary school nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "      <td>im going disagree previous comment side maltin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "      <td>one expects star trek movie high art fan expec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  ...                                     cleaned_review\n",
       "0      One of the other reviewers has mentioned that ...  ...  one reviewer mentioned watching 1 oz episode y...\n",
       "1      A wonderful little production. <br /><br />The...  ...  wonderful little production filming technique ...\n",
       "2      I thought this was a wonderful way to spend ti...  ...  thought wonderful way spend time hot summer we...\n",
       "3      Basically there's a family where a little boy ...  ...  basically there family little boy jake think t...\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  ...  petter matteis love time money visually stunni...\n",
       "...                                                  ...  ...                                                ...\n",
       "49995  I thought this movie did a down right good job...  ...  thought movie right good job wasnt creative or...\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  ...  bad plot bad dialogue bad acting idiotic direc...\n",
       "49997  I am a Catholic taught in parochial elementary...  ...  catholic taught parochial elementary school nu...\n",
       "49998  I'm going to have to disagree with the previou...  ...  im going disagree previous comment side maltin...\n",
       "49999  No one expects the Star Trek movies to be high...  ...  one expects star trek movie high art fan expec...\n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cleaned_review\"] = df.cleaned_review.progress_apply(lemmatize)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "JHjZH3ZcL73h",
    "outputId": "48e26ad4-77a9-4deb-f87f-1b82807f6fa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário após lemmatization: 210228\n"
     ]
    }
   ],
   "source": [
    "print('Tamanho do vocabulário após lemmatization:', len(get_vocab_size(df.cleaned_review)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBzaDZQVMrmr"
   },
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzwX4hk4MAsj"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "def stem(text):\n",
    "    text = text.split()\n",
    "    stemmed_text = []\n",
    "    for word in text:\n",
    "        stemmed_text.append(stemmer.stem(word))\n",
    "    return ' '.join(stemmed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "ljI_6IqMNCL4",
    "outputId": "044a3110-9269-4e46-d7a1-81ca8527b7b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [01:46<00:00, 471.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one review mention watch 1 oz episod youll hoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonder littl product film techniqu unassum old...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought wonder way spend time hot summer weeke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basic there famili littl boy jake think there ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter mattei love time money visual stun film...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought movi right good job wasnt creativ orig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>bad plot bad dialogu bad act idiot direct anno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "      <td>cathol taught parochi elementari school nun ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "      <td>im go disagre previou comment side maltin one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "      <td>one expect star trek movi high art fan expect ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  ...                                     cleaned_review\n",
       "0      One of the other reviewers has mentioned that ...  ...  one review mention watch 1 oz episod youll hoo...\n",
       "1      A wonderful little production. <br /><br />The...  ...  wonder littl product film techniqu unassum old...\n",
       "2      I thought this was a wonderful way to spend ti...  ...  thought wonder way spend time hot summer weeke...\n",
       "3      Basically there's a family where a little boy ...  ...  basic there famili littl boy jake think there ...\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  ...  petter mattei love time money visual stun film...\n",
       "...                                                  ...  ...                                                ...\n",
       "49995  I thought this movie did a down right good job...  ...  thought movi right good job wasnt creativ orig...\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  ...  bad plot bad dialogu bad act idiot direct anno...\n",
       "49997  I am a Catholic taught in parochial elementary...  ...  cathol taught parochi elementari school nun ta...\n",
       "49998  I'm going to have to disagree with the previou...  ...  im go disagre previou comment side maltin one ...\n",
       "49999  No one expects the Star Trek movies to be high...  ...  one expect star trek movi high art fan expect ...\n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cleaned_review\"] = df.cleaned_review.progress_apply(stem)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "VtOuPsibNFV6",
    "outputId": "eafd16b7-8792-4133-fcf7-920325840bd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário após stemming: 181191\n"
     ]
    }
   ],
   "source": [
    "print('Tamanho do vocabulário após stemming:', len(get_vocab_size(df.cleaned_review)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT0S7xN0PavU"
   },
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1xKyvO0PcAv"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = df.cleaned_review, df.sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5wwgsQaeiZ2"
   },
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2BkALy8b4bX"
   },
   "source": [
    "Para extração de features, serão testados três algoritmos: Tf-Idf, Hashing e Doc2Vec. Os dois primeiros se baseam apenas na frequência dos termos ao longo dos documentos para criar vetores. O terceiro utiliza aprendizado supervisionado com o objetivo de criar vetores semelhantes para documentos semelhantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRnVyFIuemBM"
   },
   "source": [
    "#### Tf-Idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUUGsg-TQCU0"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer().fit(X_train)\n",
    "train_tfidf = tfidf.transform(X_train)\n",
    "test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbKeRGgbeqH9"
   },
   "source": [
    "#### Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EfKIBIFpewGE"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "train_hv = HashingVectorizer(n_features=2**18).fit_transform(X_train)\n",
    "test_hv = HashingVectorizer(n_features=2**18).fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DP61JZ5NfRgb"
   },
   "source": [
    "#### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "P0DcCQIUnYtG",
    "outputId": "7e3029ae-fd1b-4941-e1f7-d1b6daca570c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:26<00:00, 1917.97it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "corpus = [review for review in df.cleaned_review.progress_apply(word_tokenize)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "z-pru4bxn9gA",
    "outputId": "184f017b-7437-4c22-8593-2875dd596e0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1JQMkKtehZEB"
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "tagged_corpus = [TaggedDocument(words=words, tags=[str(i)]) for i, words in enumerate(corpus)]\n",
    "model = Doc2Vec(tagged_corpus, window=3, vector_size=300, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-_Oz3mlCM6T"
   },
   "outputs": [],
   "source": [
    "train_d2v = np.array([model.docvecs[ix] for ix in X_train.index])\n",
    "test_d2v = np.array([model.docvecs[ix] for ix in X_test.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hko75kGhgveV"
   },
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HQ6xW8scnjC"
   },
   "source": [
    "No treinamento, serão usados 3 tipos de classificadores:\n",
    "* PassiveAgressive Classifier, um classificador bastante simples, com um funcionamento similar a Máquinas de Vetores de Suporte, porém com o diferencial de permitir ajustar sua agressividade (daí o nome) para ajustar o vetor de pesos em caso de uma classificação errada;\n",
    "* XGBoost, um modelo que funciona a partir de ensemble de árvores e Gradient Boosting, que basicamente tem o objetivo de, a partir de classificadores mais simples e \"fracos\" (weak \"learners\"), um classificador complexo e mais geral pode ser criado;\n",
    "* CNN, um tipo de rede neural normalmente usada em imagens, mas com aplicações para NLP também."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c6N9pjygygh"
   },
   "source": [
    "#### PassiveAgressive Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuiqwF8ahDut"
   },
   "source": [
    "##### Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "3-MNNINLPv62",
    "outputId": "4ebae696-bf90-4fdf-b846-98ab8d2d3e89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.87      0.88      5000\n",
      "    positive       0.87      0.89      0.88      5000\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pac = PassiveAggressiveClassifier(random_state=42).fit(train_tfidf, y_train)\n",
    "y_pred_pac_tfidf = pac.predict(test_tfidf)\n",
    "print(classification_report(y_test, y_pred_pac_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOD9YHjvhG1V"
   },
   "source": [
    "##### Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "vXGO5sw2hKJj",
    "outputId": "7c682e56-4335-4eb5-a41e-cb16f6dadbf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.87      0.87      5000\n",
      "    positive       0.87      0.88      0.87      5000\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pac_hv = PassiveAggressiveClassifier(random_state=42).fit(train_hv, y_train)\n",
    "y_pred_pac_hv = pac_hv.predict(test_hv)\n",
    "print(classification_report(y_test, y_pred_pac_hv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mm79EX6SCqwS"
   },
   "source": [
    "##### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "5VjWkPjHCpyD",
    "outputId": "2757f963-415f-44f2-e115-5e611715b3d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.63      0.74      5000\n",
      "    positive       0.71      0.92      0.81      5000\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.80      0.78      0.77     10000\n",
      "weighted avg       0.80      0.78      0.77     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pac_d2v = PassiveAggressiveClassifier(random_state=42).fit(train_d2v, y_train)\n",
    "y_pred_pac_d2v = pac_d2v.predict(test_d2v)\n",
    "print(classification_report(y_test, y_pred_pac_d2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPoi1vpug3pp"
   },
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n10aLom4jpvj"
   },
   "source": [
    "##### Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "qqYOYGqRRUlN",
    "outputId": "7fbe4add-9e29-4c05-c1e0-83c8c72ff2e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.82      0.84      5000\n",
      "    positive       0.83      0.86      0.84      5000\n",
      "\n",
      "    accuracy                           0.84     10000\n",
      "   macro avg       0.84      0.84      0.84     10000\n",
      "weighted avg       0.84      0.84      0.84     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(max_depth=10, random_state=42).fit(train_tfidf, y_train)\n",
    "y_pred_xgb_tfidf = xgb.predict(test_tfidf)\n",
    "print(classification_report(y_test, y_pred_xgb_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AugV7xCqj4dj"
   },
   "source": [
    "##### Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "2MqyxrIFj6mu",
    "outputId": "be2da329-1d5a-4079-fa5d-061a9ef4a6de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.82      0.84      5000\n",
      "    positive       0.83      0.86      0.84      5000\n",
      "\n",
      "    accuracy                           0.84     10000\n",
      "   macro avg       0.84      0.84      0.84     10000\n",
      "weighted avg       0.84      0.84      0.84     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_hv = XGBClassifier(max_depth=10, random_state=42).fit(train_hv, y_train)\n",
    "y_pred_xgb_hv = xgb_hv.predict(test_hv)\n",
    "print(classification_report(y_test, y_pred_xgb_hv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9X0KhSANFAp"
   },
   "source": [
    "##### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "Ndh_s2UeNDDY",
    "outputId": "d43c12f1-1ef2-4d97-8844-56f41c83bbf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.81      0.81      5000\n",
      "    positive       0.81      0.82      0.82      5000\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.82      0.82      0.82     10000\n",
      "weighted avg       0.82      0.82      0.82     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_d2v = XGBClassifier(max_depth=10, random_state=42).fit(train_d2v, y_train)\n",
    "y_pred_xgb_d2v = xgb_d2v.predict(test_d2v)\n",
    "print(classification_report(y_test, y_pred_xgb_d2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iymJJAz9d__C"
   },
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6P2vXr5Oq5sT"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAPCEkLJFtqc"
   },
   "source": [
    "Para a CNN, o vetor de entrada será criado por meio do Tokenizer oferecido pelo Keras. Ele dá um valor único para cada token presente e por meio do método text_to_sequences, transforma cada documento numa sequência desses identificadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vG0wrC9AfOPt"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=182000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "train_cnn_t2s = tokenizer.texts_to_sequences(X_train)\n",
    "test_cnn_t2s = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NU4-zGalhX41"
   },
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "\n",
    "train_cnn_t2s = pad_sequences(train_cnn_t2s, padding='post', maxlen=maxlen)\n",
    "test_cnn_t2s = pad_sequences(test_cnn_t2s, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcMotxl4hsPV"
   },
   "outputs": [],
   "source": [
    "y_train_cnn = [1 if i == \"positive\"  else 0 for i in y_train]\n",
    "y_test_cnn = [1 if i == \"positive\"  else 0 for i in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "FBGe33sWe6cS",
    "outputId": "b17419ab-756d-46d9-e53c-2d49ef913a1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          46569300  \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 96, 128)           192128    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 94, 128)           49280     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 46,819,029\n",
      "Trainable params: 46,819,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# maxlen -> Nº de dimensões dos vetor de entrada gerado após tokenizer + pad_seq\n",
    "# vocab_size -> Tamanho do vocabulário, ou seja, número de palavras distintas\n",
    "# embedding_dim -> Nº de dimensões do vetor de output da camada de Embedding\n",
    "\n",
    "embedding_dim = 300\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.Conv1D(128, 3, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "nB1K56mYhgWC",
    "outputId": "782bc51d-2f56-425c-dd8f-19be085790e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "40000/40000 [==============================] - 70s 2ms/sample - loss: 0.3701 - acc: 0.8318 - val_loss: 0.2830 - val_acc: 0.8803\n",
      "Epoch 2/10\n",
      "40000/40000 [==============================] - 71s 2ms/sample - loss: 0.1591 - acc: 0.9386 - val_loss: 0.3527 - val_acc: 0.8577\n",
      "Epoch 3/10\n",
      "40000/40000 [==============================] - 71s 2ms/sample - loss: 0.0478 - acc: 0.9833 - val_loss: 0.3921 - val_acc: 0.8722\n",
      "Epoch 4/10\n",
      "40000/40000 [==============================] - 71s 2ms/sample - loss: 0.0238 - acc: 0.9919 - val_loss: 0.5155 - val_acc: 0.8642\n",
      "Epoch 5/10\n",
      "40000/40000 [==============================] - 70s 2ms/sample - loss: 0.0165 - acc: 0.9943 - val_loss: 0.7627 - val_acc: 0.8595\n",
      "Epoch 6/10\n",
      "40000/40000 [==============================] - 70s 2ms/sample - loss: 0.0127 - acc: 0.9956 - val_loss: 0.7893 - val_acc: 0.8622\n",
      "Epoch 7/10\n",
      "40000/40000 [==============================] - 70s 2ms/sample - loss: 0.0087 - acc: 0.9971 - val_loss: 0.8293 - val_acc: 0.8652\n",
      "Epoch 8/10\n",
      "40000/40000 [==============================] - 70s 2ms/sample - loss: 0.0098 - acc: 0.9966 - val_loss: 0.8851 - val_acc: 0.8618\n",
      "Epoch 9/10\n",
      "40000/40000 [==============================] - 70s 2ms/sample - loss: 0.0084 - acc: 0.9970 - val_loss: 0.8476 - val_acc: 0.8651\n",
      "Epoch 10/10\n",
      "40000/40000 [==============================] - 70s 2ms/sample - loss: 0.0067 - acc: 0.9980 - val_loss: 0.8042 - val_acc: 0.8587\n",
      "Training Accuracy: 0.9991\n",
      "Testing Accuracy:  0.8587\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_cnn_t2s, y_train_cnn,\n",
    "                    epochs=10,\n",
    "                    validation_data=(test_cnn_t2s, y_test_cnn),\n",
    "                    batch_size=16)\n",
    "\n",
    "loss, accuracy = model.evaluate(train_cnn_t2s, y_train_cnn, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(test_cnn_t2s, y_test_cnn, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão\n",
    "\n",
    "O modelo que melhor conseguiu classificar as avaliações, tomando como base a acurácia, foi a CNN, com quase 86% no dataset de treino. Como o dado é distribuido igualmente entre as duas classes (\"positive\" e \"negative\"), essa métrica nos dá um bom parâmetro da capacidade do modelo\n",
    "\n",
    "Esse resultado nos permite inferir que, na teoria, se 100 novas avaliações fossem apresentados ao modelo, ele conseguiria determinar em quase 86 dos casos a classificação correta da avaliação. Nos outros 14%, ocorreria o que chamamos de erros do tipo I e tipo II. \n",
    "\n",
    "Em classificações binárias, como o caso desse notebook, erros do tipo I, também chamados de falso positivo, ocorrem quando uma amostra que é da classe negativa é dita como pertencente da classe positiva, ou seja, um filme que foi avaliado como ruim é considerado bom pelo modelo.\n",
    "\n",
    "Já os erros do tipo II (falsos negativos) representam a situação inversa, isto é, um filme bom é visto como ruim."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Tarefa de Casa.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
